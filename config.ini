[FilePaths]
zero_shot_prompting_result_file = ./results/zero_shot_prompting.json
prompt_template = ./data/raw/prompt_template.txt
; prepare_prompt_context_input = ./data/processed/DBLP_first_10Q.json
prepare_prompt_context_input = ./data/processed/processed_data_final500_format.json
test_data_dblp = ./data/processed/test_post_processed_data_dblp.json
prepared_data_file = ./results/prepared_data_file_zero_shot_prompting.json
zero_shot_results_file = ./results/zero_shot_prompting.json


[Token]
llamaapi = LL-toLj4vcjSC82cwlgF0yf0o5sUs8tTj8tPhbABsKMb1c74TnQud7bJbcFozHTu00n
groqapi = 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

[Model]
 model_id_1 =meta-llama/Meta-Llama-3-8B-Instruct 
 model_id_2 = meta-llama/Meta-Llama-2-7B 
 model_id_3 = mistral/Mistral-7B 

[Templates]
model_1_prompt_templates = data/raw/prompt_template2.txt 
model_2_prompt_templates = data/raw/prompt_template.txt
model_3_prompt_templates = data/raw/prompt_template.txt

[Flags]
use_api = True
api_type = groq  

[Parameters]
max_length_input = 4000
max_output_length = 50